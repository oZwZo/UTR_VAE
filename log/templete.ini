[DEFAULT]
#[TRAIN and VAL]
dataset =                     #  origin ,  masked , mix 
batch_size = 
max_epoch = 600
setp_to_check = 10

#[RUN and SETTING]
run_name = 

#[DATA]
cell_line = "A549"

#[MODEL]
model_type =                     # LSTM_AE , LSTM_VAE , Conv_AE

# optimizer
optimizer = 'Schedule'          #   'Schedule' , 'Adam' , {"weight_decay": ,"amsgrad": ,"n_warmup_steps": }
lr = 1e-3
l2 = 1e-3

#[LSTM HYPER-PARAM]
input_size = 4
hidden_size_enc = 64            # current best performance : 64 
hidden_size_dec = 64
num_layers = 2                  # current best performance : 2 with hidden =64
seq_in_dim = 4 
discretize_input = True          # True / Flase   [0.007, 0.001] [0.004, 0.01]
decode_type = "seq"
teacher_forcing =  False              # True (will decay), fixed , [t_b->float, k_b->float]
bidirectional = False
fc_output = None                 # default None , or int 

#[CONV HYPER_PARAM]
channel_ls = [4,16,32,64,128]
padding_ls = [1,0,1,0]
diliat_ls = [1,1,1,1]
kernel_size = 4                 # if 4 : [1,0,1,0] ;if 6 : pad [0,2,2,1]

#[VAE HYPER_PARAM]
latent_dim = 16
kld_weight = 

# aux tasks
Lambda = [1,1]
csv_path =   "/data/users/wergillius/UTR_VAE/multi_task/pretrain_MTL_UTR.csv"
aux_task_columns =  ['by_quantile']
